{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8719138,"sourceType":"datasetVersion","datasetId":5231641},{"sourceId":8734541,"sourceType":"datasetVersion","datasetId":5243125},{"sourceId":8735758,"sourceType":"datasetVersion","datasetId":5244078}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -r \"/kaggle/input/requirements-siamese/requirements.txt\"","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:24:57.048252Z","iopub.execute_input":"2024-06-20T11:24:57.049008Z","iopub.status.idle":"2024-06-20T11:25:10.463977Z","shell.execute_reply.started":"2024-06-20T11:24:57.048968Z","shell.execute_reply":"2024-06-20T11:25:10.462861Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (0.16.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (2024.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from __future__ import print_function\nimport argparse, random, copy\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:25:10.466065Z","iopub.execute_input":"2024-06-20T11:25:10.466368Z","iopub.status.idle":"2024-06-20T11:25:15.060894Z","shell.execute_reply.started":"2024-06-20T11:25:10.466339Z","shell.execute_reply":"2024-06-20T11:25:15.060132Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Config():\n    training_dir = \"/kaggle/input/classify-by-brand-dataset-fixed/classify_by_brand/classify_by_brand_dataset\"\n    testing_dir = \"/kaggle/input/classify-by-brand-dataset-fixed/classify_by_brand/classify_by_brand_dataset\"\n    annotations_file = \"/kaggle/working/training_dataset.csv\"\n    test_annotations_file = '/kaggle/working/test_dataset.csv'\n    train_batch_size = 64\n    train_number_epochs = 20\n    transform = transforms.Compose([        # Defining a variable transforms\n                 transforms.Resize(256),                # Resize the image to 256×256 pixels\n                 transforms.CenterCrop(224),            # Crop the image to 224×224 pixels about the center\n                 transforms.ToTensor(),                 # Convert the image to PyTorch Tensor data type\n                 transforms.Normalize(                  # Normalize the image\n                 mean=[0.485, 0.456, 0.406],            # Mean and std of image as also used when training the network\n                 std=[0.229, 0.224, 0.225]      \n            )])","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:25:16.178907Z","iopub.execute_input":"2024-06-20T11:25:16.179296Z","iopub.status.idle":"2024-06-20T11:25:16.185643Z","shell.execute_reply.started":"2024-06-20T11:25:16.179267Z","shell.execute_reply":"2024-06-20T11:25:16.184760Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Contrastive Loss definition\n# In this project Triplet loss is used which has proved to be more effective\n# for face recognition\nclass ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n\n\n        return loss_contrastive","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:56:55.375012Z","iopub.execute_input":"2024-06-20T07:56:55.375709Z","iopub.status.idle":"2024-06-20T07:56:55.382122Z","shell.execute_reply.started":"2024-06-20T07:56:55.375674Z","shell.execute_reply":"2024-06-20T07:56:55.381242Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\n\n# defining the dataset\nclass CustomImageDataset(Dataset):\n    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(label_dir)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n    \n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = Image.open(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:25:52.510498Z","iopub.execute_input":"2024-06-20T11:25:52.510899Z","iopub.status.idle":"2024-06-20T11:25:52.990472Z","shell.execute_reply.started":"2024-06-20T11:25:52.510849Z","shell.execute_reply":"2024-06-20T11:25:52.988976Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    The following is a piece of code from the pytorch Siamese Network example\n    which has been adapted to fit the needs of this project\n    The original implementation can be found at https://github.com/pytorch/examples/tree/main/siamese_network\n\"\"\"\n\nclass SiameseNetwork(nn.Module):\n    \"\"\"\n        Siamese network for image similarity estimation.\n        The network is composed of two identical networks, one for each input.\n        The output of each network is concatenated and passed to a linear layer. \n        The output of the linear layer passed through a sigmoid function.\n        `\"FaceNet\" <https://arxiv.org/pdf/1503.03832.pdf>`_ is a variant of the Siamese network.\n        This implementation varies from FaceNet as we use the `ResNet-18` model from\n        `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_ as our feature extractor.\n    \"\"\"\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        # get resnet model\n        self.resnet = torchvision.models.resnet18(weights=None)\n        \n        self.fc_in_features = self.resnet.fc.in_features\n        \n        # remove the last layer of resnet18 (linear layer which is before avgpool layer)\n        self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n\n        # add linear layers to compare between the features of the two images\n        self.fc = nn.Sequential(\n            nn.Linear(self.fc_in_features * 2, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, 1),\n        )\n\n        self.sigmoid = nn.Sigmoid()\n\n        # initialize the weights\n        self.resnet.apply(self.init_weights)\n        self.fc.apply(self.init_weights)\n        \n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight)\n            m.bias.data.fill_(0.01)\n\n    def forward_once(self, x):\n        output = self.resnet(x)\n        output = output.view(output.size()[0], -1)\n        return output\n\n    def forward(self, input1, input2, input3):\n        # get two images' features\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        output3 = self.forward_once(input3)\n\n        return output1, output2, output3\n\nclass APP_MATCHER(Dataset):\n    def __init__(self, root, train, download=False):\n        super(APP_MATCHER, self).__init__()\n\n        if train==True:\n            self.dataset = CustomImageDataset(Config.training_dir, transform=Config.transform, label_dir=Config.annotations_file)\n        else:\n            self.dataset = CustomImageDataset(Config.training_dir, transform=Config.transform, label_dir=Config.test_annotations_file)\n        \n        self.data = self.dataset.img_labels\n\n        self.group_examples()\n\n    def group_examples(self):\n        \"\"\"\n            To ease the accessibility of data based on the class, we will use `group_examples` to group \n            examples based on class. \n        \"\"\"\n\n        # get the targets from dataset\n        np_arr = np.array(self.dataset.img_labels['class_label'])\n        \n        # group examples based on class\n        self.grouped_examples = {}\n        for i in range(2):\n            self.grouped_examples[i] = np.where((np_arr==i))[0]\n    \n    def __len__(self):\n        return self.data.shape[0]\n    \n    def __getitem__(self, index):\n        \"\"\"\n            For every example, we will select two images. There are two cases, \n            positive and negative examples. For positive examples, we will have two \n            images from the same class. For negative examples, we will have two images \n            from different classes.\n        \"\"\"\n\n        # pick some random class for the first image\n        selected_class = random.randint(0, 1)\n\n        # pick a random index for the first image in the grouped indices based of the label\n        # of the class\n        random_index_1 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)\n        \n        # pick the index to get the first image\n        index_1 = self.grouped_examples[selected_class][random_index_1]\n\n        # get the first image\n        image_1 = self.dataset.__getitem__(index_1)[0]\n\n        # same class\n        # pick a random index for the second image\n        random_index_2 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)\n\n        # ensure that the index of the second image isn't the same as the first image\n        while random_index_2 == random_index_1:\n            random_index_2 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)\n\n        # pick the index to get the second image\n        index_2 = self.grouped_examples[selected_class][random_index_2]\n\n        # get the second image\n        image_2 = self.dataset.__getitem__(index_2)[0]\n        \n        # different class=\n        other_selected_class = 1-selected_class\n\n        # ensure that the class of the second image isn't the same as the first image\n\n\n        # pick a random index for the second image in the grouped indices based of the label\n        # of the class\n        random_index_3 = random.randint(0, self.grouped_examples[other_selected_class].shape[0]-1)\n\n        # pick the index to get the second image\n        index_3 = self.grouped_examples[other_selected_class][random_index_3]\n\n        # get the second image\n        image_3 = self.dataset.__getitem__(index_3)[0]\n\n        return image_1, image_2, image_3\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n\n    criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n\n    for batch_idx, (images_1, images_2, images_3) in enumerate(train_loader):\n        images_1, images_2, images_3 = images_1.to(device), images_2.to(device), images_3.to(device)\n        optimizer.zero_grad()\n        outputs = model(images_1, images_2, images_3)\n        loss = criterion(outputs[0],outputs[1], outputs[2])\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(images_1), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n\n    with torch.no_grad():\n        for (images_1, images_2, images_3) in test_loader:\n            images_1, images_2, images_3 = images_1.to(device), images_2.to(device), images_3.to(device)\n            outputs = model(images_1, images_2, images_3)\n            test_loss += criterion(outputs[0],outputs[1], outputs[2]).sum().item()  # sum up batch loss\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\ndef main():\n    # Training settings\n    class Args(argparse.Namespace):\n        batch_size=64\n        test_batch_size=64\n        epochs=20\n        lr=0.005\n        gamma=0.7\n        no_cuda=False\n        no_mps=True\n        dry_run=False\n        seed=1\n        log_interval=10\n        save_model=True\n\n    args=Args()\n    \n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n    torch.manual_seed(args.seed)\n\n    if use_cuda:\n        device = torch.device(\"cuda\")\n    elif use_mps:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    train_kwargs = {'batch_size': args.batch_size}\n    test_kwargs = {'batch_size': args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    train_dataset = APP_MATCHER('../data', train=True, download=True)\n    test_dataset = APP_MATCHER('../data', train=False)\n    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)\n\n    model = SiameseNetwork().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n#         test(model, device, test_loader)\n        scheduler.step()\n\n    if args.save_model:\n        torch.save(model.state_dict(), \"siamese_network.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:30:12.192096Z","iopub.execute_input":"2024-06-20T11:30:12.192418Z","iopub.status.idle":"2024-06-20T11:30:12.230625Z","shell.execute_reply.started":"2024-06-20T11:30:12.192388Z","shell.execute_reply":"2024-06-20T11:30:12.229688Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:30:12.232487Z","iopub.execute_input":"2024-06-20T11:30:12.232751Z","iopub.status.idle":"2024-06-20T12:03:23.784679Z","shell.execute_reply.started":"2024-06-20T11:30:12.232730Z","shell.execute_reply":"2024-06-20T12:03:23.783588Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Train Epoch: 1 [0/7723 (0%)]\tLoss: 2.508582\nTrain Epoch: 1 [640/7723 (8%)]\tLoss: 1.484504\nTrain Epoch: 1 [1280/7723 (17%)]\tLoss: 1.923212\nTrain Epoch: 1 [1920/7723 (25%)]\tLoss: 1.219400\nTrain Epoch: 1 [2560/7723 (33%)]\tLoss: 1.204297\nTrain Epoch: 1 [3200/7723 (41%)]\tLoss: 0.978742\nTrain Epoch: 1 [3840/7723 (50%)]\tLoss: 1.080977\nTrain Epoch: 1 [4480/7723 (58%)]\tLoss: 1.127035\nTrain Epoch: 1 [5120/7723 (66%)]\tLoss: 1.199586\nTrain Epoch: 1 [5760/7723 (74%)]\tLoss: 1.293611\nTrain Epoch: 1 [6400/7723 (83%)]\tLoss: 0.703592\nTrain Epoch: 1 [7040/7723 (91%)]\tLoss: 0.969760\nTrain Epoch: 1 [5160/7723 (99%)]\tLoss: 0.790864\nTrain Epoch: 2 [0/7723 (0%)]\tLoss: 0.734059\nTrain Epoch: 2 [640/7723 (8%)]\tLoss: 0.824094\nTrain Epoch: 2 [1280/7723 (17%)]\tLoss: 0.732086\nTrain Epoch: 2 [1920/7723 (25%)]\tLoss: 0.672383\nTrain Epoch: 2 [2560/7723 (33%)]\tLoss: 0.819933\nTrain Epoch: 2 [3200/7723 (41%)]\tLoss: 0.471012\nTrain Epoch: 2 [3840/7723 (50%)]\tLoss: 0.568180\nTrain Epoch: 2 [4480/7723 (58%)]\tLoss: 0.685142\nTrain Epoch: 2 [5120/7723 (66%)]\tLoss: 0.629840\nTrain Epoch: 2 [5760/7723 (74%)]\tLoss: 0.389061\nTrain Epoch: 2 [6400/7723 (83%)]\tLoss: 0.655945\nTrain Epoch: 2 [7040/7723 (91%)]\tLoss: 0.552898\nTrain Epoch: 2 [5160/7723 (99%)]\tLoss: 0.554429\nTrain Epoch: 3 [0/7723 (0%)]\tLoss: 0.583825\nTrain Epoch: 3 [640/7723 (8%)]\tLoss: 0.593366\nTrain Epoch: 3 [1280/7723 (17%)]\tLoss: 0.648708\nTrain Epoch: 3 [1920/7723 (25%)]\tLoss: 0.741318\nTrain Epoch: 3 [2560/7723 (33%)]\tLoss: 0.548734\nTrain Epoch: 3 [3200/7723 (41%)]\tLoss: 0.426206\nTrain Epoch: 3 [3840/7723 (50%)]\tLoss: 0.843520\nTrain Epoch: 3 [4480/7723 (58%)]\tLoss: 0.476074\nTrain Epoch: 3 [5120/7723 (66%)]\tLoss: 0.462022\nTrain Epoch: 3 [5760/7723 (74%)]\tLoss: 0.553180\nTrain Epoch: 3 [6400/7723 (83%)]\tLoss: 0.559699\nTrain Epoch: 3 [7040/7723 (91%)]\tLoss: 0.572091\nTrain Epoch: 3 [5160/7723 (99%)]\tLoss: 0.597833\nTrain Epoch: 4 [0/7723 (0%)]\tLoss: 0.393774\nTrain Epoch: 4 [640/7723 (8%)]\tLoss: 0.398739\nTrain Epoch: 4 [1280/7723 (17%)]\tLoss: 0.562379\nTrain Epoch: 4 [1920/7723 (25%)]\tLoss: 0.355969\nTrain Epoch: 4 [2560/7723 (33%)]\tLoss: 0.626517\nTrain Epoch: 4 [3200/7723 (41%)]\tLoss: 0.321074\nTrain Epoch: 4 [3840/7723 (50%)]\tLoss: 0.400522\nTrain Epoch: 4 [4480/7723 (58%)]\tLoss: 0.465707\nTrain Epoch: 4 [5120/7723 (66%)]\tLoss: 0.438058\nTrain Epoch: 4 [5760/7723 (74%)]\tLoss: 0.371203\nTrain Epoch: 4 [6400/7723 (83%)]\tLoss: 0.412812\nTrain Epoch: 4 [7040/7723 (91%)]\tLoss: 0.392388\nTrain Epoch: 4 [5160/7723 (99%)]\tLoss: 0.618864\nTrain Epoch: 5 [0/7723 (0%)]\tLoss: 0.529091\nTrain Epoch: 5 [640/7723 (8%)]\tLoss: 0.436897\nTrain Epoch: 5 [1280/7723 (17%)]\tLoss: 0.742194\nTrain Epoch: 5 [1920/7723 (25%)]\tLoss: 0.453900\nTrain Epoch: 5 [2560/7723 (33%)]\tLoss: 0.596418\nTrain Epoch: 5 [3200/7723 (41%)]\tLoss: 0.367265\nTrain Epoch: 5 [3840/7723 (50%)]\tLoss: 0.277454\nTrain Epoch: 5 [4480/7723 (58%)]\tLoss: 0.453592\nTrain Epoch: 5 [5120/7723 (66%)]\tLoss: 0.283243\nTrain Epoch: 5 [5760/7723 (74%)]\tLoss: 0.314177\nTrain Epoch: 5 [6400/7723 (83%)]\tLoss: 0.460542\nTrain Epoch: 5 [7040/7723 (91%)]\tLoss: 0.458527\nTrain Epoch: 5 [5160/7723 (99%)]\tLoss: 0.609149\nTrain Epoch: 6 [0/7723 (0%)]\tLoss: 0.360761\nTrain Epoch: 6 [640/7723 (8%)]\tLoss: 0.483988\nTrain Epoch: 6 [1280/7723 (17%)]\tLoss: 0.405186\nTrain Epoch: 6 [1920/7723 (25%)]\tLoss: 0.264447\nTrain Epoch: 6 [2560/7723 (33%)]\tLoss: 0.407852\nTrain Epoch: 6 [3200/7723 (41%)]\tLoss: 0.639003\nTrain Epoch: 6 [3840/7723 (50%)]\tLoss: 0.313371\nTrain Epoch: 6 [4480/7723 (58%)]\tLoss: 0.515510\nTrain Epoch: 6 [5120/7723 (66%)]\tLoss: 0.404159\nTrain Epoch: 6 [5760/7723 (74%)]\tLoss: 0.353892\nTrain Epoch: 6 [6400/7723 (83%)]\tLoss: 0.390370\nTrain Epoch: 6 [7040/7723 (91%)]\tLoss: 0.440727\nTrain Epoch: 6 [5160/7723 (99%)]\tLoss: 0.404144\nTrain Epoch: 7 [0/7723 (0%)]\tLoss: 0.659395\nTrain Epoch: 7 [640/7723 (8%)]\tLoss: 0.300715\nTrain Epoch: 7 [1280/7723 (17%)]\tLoss: 0.481676\nTrain Epoch: 7 [1920/7723 (25%)]\tLoss: 0.339756\nTrain Epoch: 7 [2560/7723 (33%)]\tLoss: 0.290418\nTrain Epoch: 7 [3200/7723 (41%)]\tLoss: 0.509265\nTrain Epoch: 7 [3840/7723 (50%)]\tLoss: 0.292357\nTrain Epoch: 7 [4480/7723 (58%)]\tLoss: 0.672963\nTrain Epoch: 7 [5120/7723 (66%)]\tLoss: 0.618298\nTrain Epoch: 7 [5760/7723 (74%)]\tLoss: 0.318625\nTrain Epoch: 7 [6400/7723 (83%)]\tLoss: 0.381701\nTrain Epoch: 7 [7040/7723 (91%)]\tLoss: 0.652865\nTrain Epoch: 7 [5160/7723 (99%)]\tLoss: 0.475244\nTrain Epoch: 8 [0/7723 (0%)]\tLoss: 0.335129\nTrain Epoch: 8 [640/7723 (8%)]\tLoss: 0.524679\nTrain Epoch: 8 [1280/7723 (17%)]\tLoss: 0.363999\nTrain Epoch: 8 [1920/7723 (25%)]\tLoss: 0.445094\nTrain Epoch: 8 [2560/7723 (33%)]\tLoss: 0.339659\nTrain Epoch: 8 [3200/7723 (41%)]\tLoss: 0.445252\nTrain Epoch: 8 [3840/7723 (50%)]\tLoss: 0.482775\nTrain Epoch: 8 [4480/7723 (58%)]\tLoss: 0.494591\nTrain Epoch: 8 [5120/7723 (66%)]\tLoss: 0.213942\nTrain Epoch: 8 [5760/7723 (74%)]\tLoss: 0.432298\nTrain Epoch: 8 [6400/7723 (83%)]\tLoss: 0.199554\nTrain Epoch: 8 [7040/7723 (91%)]\tLoss: 0.458225\nTrain Epoch: 8 [5160/7723 (99%)]\tLoss: 0.273144\nTrain Epoch: 9 [0/7723 (0%)]\tLoss: 0.322235\nTrain Epoch: 9 [640/7723 (8%)]\tLoss: 0.551450\nTrain Epoch: 9 [1280/7723 (17%)]\tLoss: 0.427410\nTrain Epoch: 9 [1920/7723 (25%)]\tLoss: 0.421342\nTrain Epoch: 9 [2560/7723 (33%)]\tLoss: 0.392065\nTrain Epoch: 9 [3200/7723 (41%)]\tLoss: 0.249440\nTrain Epoch: 9 [3840/7723 (50%)]\tLoss: 0.405328\nTrain Epoch: 9 [4480/7723 (58%)]\tLoss: 0.279249\nTrain Epoch: 9 [5120/7723 (66%)]\tLoss: 0.210348\nTrain Epoch: 9 [5760/7723 (74%)]\tLoss: 0.218122\nTrain Epoch: 9 [6400/7723 (83%)]\tLoss: 0.559900\nTrain Epoch: 9 [7040/7723 (91%)]\tLoss: 0.263804\nTrain Epoch: 9 [5160/7723 (99%)]\tLoss: 0.442356\nTrain Epoch: 10 [0/7723 (0%)]\tLoss: 0.486595\nTrain Epoch: 10 [640/7723 (8%)]\tLoss: 0.177972\nTrain Epoch: 10 [1280/7723 (17%)]\tLoss: 0.256308\nTrain Epoch: 10 [1920/7723 (25%)]\tLoss: 0.463252\nTrain Epoch: 10 [2560/7723 (33%)]\tLoss: 0.340500\nTrain Epoch: 10 [3200/7723 (41%)]\tLoss: 0.420330\nTrain Epoch: 10 [3840/7723 (50%)]\tLoss: 0.558476\nTrain Epoch: 10 [4480/7723 (58%)]\tLoss: 0.415816\nTrain Epoch: 10 [5120/7723 (66%)]\tLoss: 0.441723\nTrain Epoch: 10 [5760/7723 (74%)]\tLoss: 0.288866\nTrain Epoch: 10 [6400/7723 (83%)]\tLoss: 0.349273\nTrain Epoch: 10 [7040/7723 (91%)]\tLoss: 0.461483\nTrain Epoch: 10 [5160/7723 (99%)]\tLoss: 0.515209\nTrain Epoch: 11 [0/7723 (0%)]\tLoss: 0.342274\nTrain Epoch: 11 [640/7723 (8%)]\tLoss: 0.293748\nTrain Epoch: 11 [1280/7723 (17%)]\tLoss: 0.392583\nTrain Epoch: 11 [1920/7723 (25%)]\tLoss: 0.333826\nTrain Epoch: 11 [2560/7723 (33%)]\tLoss: 0.399736\nTrain Epoch: 11 [3200/7723 (41%)]\tLoss: 0.521758\nTrain Epoch: 11 [3840/7723 (50%)]\tLoss: 0.265814\nTrain Epoch: 11 [4480/7723 (58%)]\tLoss: 0.184433\nTrain Epoch: 11 [5120/7723 (66%)]\tLoss: 0.389191\nTrain Epoch: 11 [5760/7723 (74%)]\tLoss: 0.435830\nTrain Epoch: 11 [6400/7723 (83%)]\tLoss: 0.407453\nTrain Epoch: 11 [7040/7723 (91%)]\tLoss: 0.440413\nTrain Epoch: 11 [5160/7723 (99%)]\tLoss: 0.529818\nTrain Epoch: 12 [0/7723 (0%)]\tLoss: 0.445421\nTrain Epoch: 12 [640/7723 (8%)]\tLoss: 0.361023\nTrain Epoch: 12 [1280/7723 (17%)]\tLoss: 0.417088\nTrain Epoch: 12 [1920/7723 (25%)]\tLoss: 0.503054\nTrain Epoch: 12 [2560/7723 (33%)]\tLoss: 0.488247\nTrain Epoch: 12 [3200/7723 (41%)]\tLoss: 0.312711\nTrain Epoch: 12 [3840/7723 (50%)]\tLoss: 0.517630\nTrain Epoch: 12 [4480/7723 (58%)]\tLoss: 0.380306\nTrain Epoch: 12 [5120/7723 (66%)]\tLoss: 0.388104\nTrain Epoch: 12 [5760/7723 (74%)]\tLoss: 0.201880\nTrain Epoch: 12 [6400/7723 (83%)]\tLoss: 0.238996\nTrain Epoch: 12 [7040/7723 (91%)]\tLoss: 0.341624\nTrain Epoch: 12 [5160/7723 (99%)]\tLoss: 0.508353\nTrain Epoch: 13 [0/7723 (0%)]\tLoss: 0.505377\nTrain Epoch: 13 [640/7723 (8%)]\tLoss: 0.265329\nTrain Epoch: 13 [1280/7723 (17%)]\tLoss: 0.466354\nTrain Epoch: 13 [1920/7723 (25%)]\tLoss: 0.332551\nTrain Epoch: 13 [2560/7723 (33%)]\tLoss: 0.313553\nTrain Epoch: 13 [3200/7723 (41%)]\tLoss: 0.488301\nTrain Epoch: 13 [3840/7723 (50%)]\tLoss: 0.312728\nTrain Epoch: 13 [4480/7723 (58%)]\tLoss: 0.453671\nTrain Epoch: 13 [5120/7723 (66%)]\tLoss: 0.327165\nTrain Epoch: 13 [5760/7723 (74%)]\tLoss: 0.344702\nTrain Epoch: 13 [6400/7723 (83%)]\tLoss: 0.395007\nTrain Epoch: 13 [7040/7723 (91%)]\tLoss: 0.454018\nTrain Epoch: 13 [5160/7723 (99%)]\tLoss: 0.346814\nTrain Epoch: 14 [0/7723 (0%)]\tLoss: 0.258334\nTrain Epoch: 14 [640/7723 (8%)]\tLoss: 0.252716\nTrain Epoch: 14 [1280/7723 (17%)]\tLoss: 0.401359\nTrain Epoch: 14 [1920/7723 (25%)]\tLoss: 0.244455\nTrain Epoch: 14 [2560/7723 (33%)]\tLoss: 0.409664\nTrain Epoch: 14 [3200/7723 (41%)]\tLoss: 0.357407\nTrain Epoch: 14 [3840/7723 (50%)]\tLoss: 0.261101\nTrain Epoch: 14 [4480/7723 (58%)]\tLoss: 0.468230\nTrain Epoch: 14 [5120/7723 (66%)]\tLoss: 0.291684\nTrain Epoch: 14 [5760/7723 (74%)]\tLoss: 0.297115\nTrain Epoch: 14 [6400/7723 (83%)]\tLoss: 0.315602\nTrain Epoch: 14 [7040/7723 (91%)]\tLoss: 0.247424\nTrain Epoch: 14 [5160/7723 (99%)]\tLoss: 0.491310\nTrain Epoch: 15 [0/7723 (0%)]\tLoss: 0.326031\nTrain Epoch: 15 [640/7723 (8%)]\tLoss: 0.223109\nTrain Epoch: 15 [1280/7723 (17%)]\tLoss: 0.275705\nTrain Epoch: 15 [1920/7723 (25%)]\tLoss: 0.370870\nTrain Epoch: 15 [2560/7723 (33%)]\tLoss: 0.263667\nTrain Epoch: 15 [3200/7723 (41%)]\tLoss: 0.479869\nTrain Epoch: 15 [3840/7723 (50%)]\tLoss: 0.260113\nTrain Epoch: 15 [4480/7723 (58%)]\tLoss: 0.289705\nTrain Epoch: 15 [5120/7723 (66%)]\tLoss: 0.372076\nTrain Epoch: 15 [5760/7723 (74%)]\tLoss: 0.391920\nTrain Epoch: 15 [6400/7723 (83%)]\tLoss: 0.214701\nTrain Epoch: 15 [7040/7723 (91%)]\tLoss: 0.312053\nTrain Epoch: 15 [5160/7723 (99%)]\tLoss: 0.356132\nTrain Epoch: 16 [0/7723 (0%)]\tLoss: 0.483692\nTrain Epoch: 16 [640/7723 (8%)]\tLoss: 0.350195\nTrain Epoch: 16 [1280/7723 (17%)]\tLoss: 0.308660\nTrain Epoch: 16 [1920/7723 (25%)]\tLoss: 0.433910\nTrain Epoch: 16 [2560/7723 (33%)]\tLoss: 0.337041\nTrain Epoch: 16 [3200/7723 (41%)]\tLoss: 0.328972\nTrain Epoch: 16 [3840/7723 (50%)]\tLoss: 0.499344\nTrain Epoch: 16 [4480/7723 (58%)]\tLoss: 0.176564\nTrain Epoch: 16 [5120/7723 (66%)]\tLoss: 0.416806\nTrain Epoch: 16 [5760/7723 (74%)]\tLoss: 0.305389\nTrain Epoch: 16 [6400/7723 (83%)]\tLoss: 0.366211\nTrain Epoch: 16 [7040/7723 (91%)]\tLoss: 0.310061\nTrain Epoch: 16 [5160/7723 (99%)]\tLoss: 0.354146\nTrain Epoch: 17 [0/7723 (0%)]\tLoss: 0.342699\nTrain Epoch: 17 [640/7723 (8%)]\tLoss: 0.503189\nTrain Epoch: 17 [1280/7723 (17%)]\tLoss: 0.316622\nTrain Epoch: 17 [1920/7723 (25%)]\tLoss: 0.511896\nTrain Epoch: 17 [2560/7723 (33%)]\tLoss: 0.405538\nTrain Epoch: 17 [3200/7723 (41%)]\tLoss: 0.353703\nTrain Epoch: 17 [3840/7723 (50%)]\tLoss: 0.359854\nTrain Epoch: 17 [4480/7723 (58%)]\tLoss: 0.345901\nTrain Epoch: 17 [5120/7723 (66%)]\tLoss: 0.275796\nTrain Epoch: 17 [5760/7723 (74%)]\tLoss: 0.490194\nTrain Epoch: 17 [6400/7723 (83%)]\tLoss: 0.388306\nTrain Epoch: 17 [7040/7723 (91%)]\tLoss: 0.193812\nTrain Epoch: 17 [5160/7723 (99%)]\tLoss: 0.514172\nTrain Epoch: 18 [0/7723 (0%)]\tLoss: 0.311100\nTrain Epoch: 18 [640/7723 (8%)]\tLoss: 0.551837\nTrain Epoch: 18 [1280/7723 (17%)]\tLoss: 0.383851\nTrain Epoch: 18 [1920/7723 (25%)]\tLoss: 0.367848\nTrain Epoch: 18 [2560/7723 (33%)]\tLoss: 0.261667\nTrain Epoch: 18 [3200/7723 (41%)]\tLoss: 0.379151\nTrain Epoch: 18 [3840/7723 (50%)]\tLoss: 0.472429\nTrain Epoch: 18 [4480/7723 (58%)]\tLoss: 0.409220\nTrain Epoch: 18 [5120/7723 (66%)]\tLoss: 0.224161\nTrain Epoch: 18 [5760/7723 (74%)]\tLoss: 0.342147\nTrain Epoch: 18 [6400/7723 (83%)]\tLoss: 0.384102\nTrain Epoch: 18 [7040/7723 (91%)]\tLoss: 0.414629\nTrain Epoch: 18 [5160/7723 (99%)]\tLoss: 0.153040\nTrain Epoch: 19 [0/7723 (0%)]\tLoss: 0.518705\nTrain Epoch: 19 [640/7723 (8%)]\tLoss: 0.298606\nTrain Epoch: 19 [1280/7723 (17%)]\tLoss: 0.507584\nTrain Epoch: 19 [1920/7723 (25%)]\tLoss: 0.446056\nTrain Epoch: 19 [2560/7723 (33%)]\tLoss: 0.353242\nTrain Epoch: 19 [3200/7723 (41%)]\tLoss: 0.341226\nTrain Epoch: 19 [3840/7723 (50%)]\tLoss: 0.273697\nTrain Epoch: 19 [4480/7723 (58%)]\tLoss: 0.338749\nTrain Epoch: 19 [5120/7723 (66%)]\tLoss: 0.519639\nTrain Epoch: 19 [5760/7723 (74%)]\tLoss: 0.329358\nTrain Epoch: 19 [6400/7723 (83%)]\tLoss: 0.705858\nTrain Epoch: 19 [7040/7723 (91%)]\tLoss: 0.489936\nTrain Epoch: 19 [5160/7723 (99%)]\tLoss: 0.456400\nTrain Epoch: 20 [0/7723 (0%)]\tLoss: 0.606557\nTrain Epoch: 20 [640/7723 (8%)]\tLoss: 0.299404\nTrain Epoch: 20 [1280/7723 (17%)]\tLoss: 0.368288\nTrain Epoch: 20 [1920/7723 (25%)]\tLoss: 0.319540\nTrain Epoch: 20 [2560/7723 (33%)]\tLoss: 0.384475\nTrain Epoch: 20 [3200/7723 (41%)]\tLoss: 0.439358\nTrain Epoch: 20 [3840/7723 (50%)]\tLoss: 0.313848\nTrain Epoch: 20 [4480/7723 (58%)]\tLoss: 0.315851\nTrain Epoch: 20 [5120/7723 (66%)]\tLoss: 0.336559\nTrain Epoch: 20 [5760/7723 (74%)]\tLoss: 0.337882\nTrain Epoch: 20 [6400/7723 (83%)]\tLoss: 0.364628\nTrain Epoch: 20 [7040/7723 (91%)]\tLoss: 0.428135\nTrain Epoch: 20 [5160/7723 (99%)]\tLoss: 0.416075\n","output_type":"stream"}]}]}