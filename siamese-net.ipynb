{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training the Siamese Network\n","The Siamese Network is a neural network architecture used for image similarity estimation. It consists of two identical networks, each taking an input image and producing a feature vector. The feature vectors are then compared using a contrastive loss function or more recently a Triple Loss function to determine the similarity between the images. The network is trained using a dataset of image pairs, where positive pairs are from the same class and negative pairs are from different classes. The training process involves optimizing the network's parameters to minimize the contrastive loss."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:24:57.049008Z","iopub.status.busy":"2024-06-20T11:24:57.048252Z","iopub.status.idle":"2024-06-20T11:25:10.463977Z","shell.execute_reply":"2024-06-20T11:25:10.462861Z","shell.execute_reply.started":"2024-06-20T11:24:57.048968Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (2.1.2)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (0.16.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (2024.3.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (1.26.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r /kaggle/input/requirements-siamese/requirements.txt (line 2)) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r /kaggle/input/requirements-siamese/requirements.txt (line 1)) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -r \"/kaggle/input/requirements-siamese/requirements.txt\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:25:10.466368Z","iopub.status.busy":"2024-06-20T11:25:10.466065Z","iopub.status.idle":"2024-06-20T11:25:15.060894Z","shell.execute_reply":"2024-06-20T11:25:15.060132Z","shell.execute_reply.started":"2024-06-20T11:25:10.466339Z"},"trusted":true},"outputs":[],"source":["from __future__ import print_function\n","import argparse, random, copy\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:25:16.179296Z","iopub.status.busy":"2024-06-20T11:25:16.178907Z","iopub.status.idle":"2024-06-20T11:25:16.185643Z","shell.execute_reply":"2024-06-20T11:25:16.184760Z","shell.execute_reply.started":"2024-06-20T11:25:16.179267Z"},"trusted":true},"outputs":[],"source":["class Config():\n","    training_dir = \"/kaggle/input/classify-by-brand-dataset-fixed/classify_by_brand/classify_by_brand_dataset\"\n","    testing_dir = \"/kaggle/input/classify-by-brand-dataset-fixed/classify_by_brand/classify_by_brand_dataset\"\n","    annotations_file = \"/kaggle/working/training_dataset.csv\"\n","    test_annotations_file = '/kaggle/working/test_dataset.csv'\n","    train_batch_size = 64\n","    train_number_epochs = 20\n","    transform = transforms.Compose([        # Defining a variable transforms\n","                 transforms.Resize(256),                # Resize the image to 256×256 pixels\n","                 transforms.CenterCrop(224),            # Crop the image to 224×224 pixels about the center\n","                 transforms.ToTensor(),                 # Convert the image to PyTorch Tensor data type\n","                 transforms.Normalize(                  # Normalize the image\n","                 mean=[0.485, 0.456, 0.406],            # Mean and std of image as also used when training the network\n","                 std=[0.229, 0.224, 0.225]      \n","            )])\n","    NUM_CLASSES = 10"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T07:56:55.375709Z","iopub.status.busy":"2024-06-20T07:56:55.375012Z","iopub.status.idle":"2024-06-20T07:56:55.382122Z","shell.execute_reply":"2024-06-20T07:56:55.381242Z","shell.execute_reply.started":"2024-06-20T07:56:55.375674Z"},"trusted":true},"outputs":[],"source":["# Contrastive Loss definition\n","# In this project Triplet loss is used which has proved to be more effective\n","# for face recognition\n","class ContrastiveLoss(torch.nn.Module):\n","    \"\"\"\n","    Contrastive loss function.\n","    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n","    \"\"\"\n","\n","    def __init__(self, margin=2.0):\n","        super(ContrastiveLoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, output1, output2, label):\n","        euclidean_distance = F.pairwise_distance(output1, output2)\n","        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n","                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n","\n","\n","        return loss_contrastive"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:25:52.510899Z","iopub.status.busy":"2024-06-20T11:25:52.510498Z","iopub.status.idle":"2024-06-20T11:25:52.990472Z","shell.execute_reply":"2024-06-20T11:25:52.988976Z","shell.execute_reply.started":"2024-06-20T11:25:52.510849Z"},"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","from PIL import Image\n","\n","# defining the dataset\n","class CustomImageDataset(Dataset):\n","    def __init__(self, img_dir, label_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(label_dir)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","    \n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = Image.open(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:30:12.192418Z","iopub.status.busy":"2024-06-20T11:30:12.192096Z","iopub.status.idle":"2024-06-20T11:30:12.230625Z","shell.execute_reply":"2024-06-20T11:30:12.229688Z","shell.execute_reply.started":"2024-06-20T11:30:12.192388Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","    The following is a piece of code from the pytorch Siamese Network example\n","    which has been adapted to fit the needs of this project\n","    The original implementation can be found at https://github.com/pytorch/examples/tree/main/siamese_network\n","\"\"\"\n","\n","class SiameseNetwork(nn.Module):\n","    \"\"\"\n","        Siamese network for image similarity estimation.\n","        The network is composed of two identical networks, one for each input.\n","        The output of each network is concatenated and passed to a linear layer. \n","        The output of the linear layer passed through a sigmoid function.\n","        `\"FaceNet\" <https://arxiv.org/pdf/1503.03832.pdf>`_ is a variant of the Siamese network.\n","        This implementation varies from FaceNet as we use the `ResNet-18` model from\n","        `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_ as our feature extractor.\n","    \"\"\"\n","    def __init__(self):\n","        super(SiameseNetwork, self).__init__()\n","        # get resnet model\n","        self.resnet = torchvision.models.resnet18(weights=None)\n","        \n","        self.fc_in_features = self.resnet.fc.in_features\n","        \n","        # remove the last layer of resnet18 (linear layer which is before avgpool layer)\n","        self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n","\n","        # add linear layers to compare between the features of the two images\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.fc_in_features * 2, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 1),\n","        )\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","        # initialize the weights\n","        self.resnet.apply(self.init_weights)\n","        self.fc.apply(self.init_weights)\n","        \n","    def init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            torch.nn.init.xavier_uniform_(m.weight)\n","            m.bias.data.fill_(0.01)\n","\n","    def forward_once(self, x):\n","        output = self.resnet(x)\n","        output = output.view(output.size()[0], -1)\n","        return output\n","\n","    def forward(self, input1, input2, input3):\n","        # get two images' features\n","        output1 = self.forward_once(input1)\n","        output2 = self.forward_once(input2)\n","        output3 = self.forward_once(input3)\n","\n","        return output1, output2, output3\n","\n","class APP_MATCHER(Dataset):\n","    def __init__(self, root, train, download=False):\n","        super(APP_MATCHER, self).__init__()\n","\n","        if train==True:\n","            self.dataset = CustomImageDataset(Config.training_dir, transform=Config.transform, label_dir=Config.annotations_file)\n","        else:\n","            self.dataset = CustomImageDataset(Config.training_dir, transform=Config.transform, label_dir=Config.test_annotations_file)\n","        \n","        self.data = self.dataset.img_labels\n","\n","        self.group_examples()\n","\n","    def group_examples(self):\n","        \"\"\"\n","            To ease the accessibility of data based on the class, we will use `group_examples` to group \n","            examples based on class. \n","        \"\"\"\n","\n","        # get the targets from dataset\n","        np_arr = np.array(self.dataset.img_labels['class_label'])\n","        \n","        # group examples based on class\n","        self.grouped_examples = {}\n","        for i in range(2):\n","            self.grouped_examples[i] = np.where((np_arr==i))[0]\n","    \n","    def __len__(self):\n","        return self.data.shape[0]\n","    \n","    def __getitem__(self, index):\n","        \"\"\"\n","            For every example, we will select two images. There are two cases, \n","            positive and negative examples. For positive examples, we will have two \n","            images from the same class. For negative examples, we will have two images \n","            from different classes.\n","        \"\"\"\n","\n","        # pick some random class for the first image\n","        selected_class = random.randint(0, Config.NUM_CLASSES-1)\n","\n","        # pick a random index for the first image in the grouped indices based of the label\n","        # of the class\n","        random_index_1 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)\n","        \n","        # pick the index to get the first image\n","        index_1 = self.grouped_examples[selected_class][random_index_1]\n","\n","        # get the first image\n","        image_1 = self.dataset.__getitem__(index_1)[0]\n","\n","        # same class\n","        # pick a random index for the second image\n","        random_index_2 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)\n","\n","        # ensure that the index of the second image isn't the same as the first image\n","        while random_index_2 == random_index_1:\n","            random_index_2 = random.randint(0, self.grouped_examples[selected_class].shape[0]-1)\n","\n","        # pick the index to get the second image\n","        index_2 = self.grouped_examples[selected_class][random_index_2]\n","\n","        # get the second image\n","        image_2 = self.dataset.__getitem__(index_2)[0]\n","        \n","        # different class=\n","        other_selected_class = random.randint(0, Config.NUM_CLASSES-1)\n","\n","        # ensure that the class of the second image isn't the same as the first image\n","        while other_selected_class == selected_class:\n","            other_selected_class = random.randint(0, Config.NUM_CLASSES-1)\n","\n","        # pick a random index for the second image in the grouped indices based of the label\n","        # of the class\n","        random_index_3 = random.randint(0, self.grouped_examples[other_selected_class].shape[0]-1)\n","\n","        # pick the index to get the second image\n","        index_3 = self.grouped_examples[other_selected_class][random_index_3]\n","\n","        # get the second image\n","        image_3 = self.dataset.__getitem__(index_3)[0]\n","\n","        return image_1, image_2, image_3\n","\n","\n","def train(args, model, device, train_loader, optimizer, epoch):\n","    model.train()\n","\n","    criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n","\n","    for batch_idx, (images_1, images_2, images_3) in enumerate(train_loader):\n","        images_1, images_2, images_3 = images_1.to(device), images_2.to(device), images_3.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images_1, images_2, images_3)\n","        loss = criterion(outputs[0],outputs[1], outputs[2])\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(images_1), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            if args.dry_run:\n","                break\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","\n","    criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n","\n","    with torch.no_grad():\n","        for (images_1, images_2, images_3) in test_loader:\n","            images_1, images_2, images_3 = images_1.to(device), images_2.to(device), images_3.to(device)\n","            outputs = model(images_1, images_2, images_3)\n","            test_loss += criterion(outputs[0],outputs[1], outputs[2]).sum().item()  # sum up batch loss\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","\n","def main():\n","    # Training settings\n","    class Args(argparse.Namespace):\n","        batch_size=64\n","        test_batch_size=64\n","        epochs=20\n","        lr=0.005\n","        gamma=0.7\n","        no_cuda=False\n","        no_mps=True\n","        dry_run=False\n","        seed=1\n","        log_interval=10\n","        save_model=True\n","\n","    args=Args()\n","    \n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","    use_mps = not args.no_mps and torch.backends.mps.is_available()\n","\n","    torch.manual_seed(args.seed)\n","\n","    if use_cuda:\n","        device = torch.device(\"cuda\")\n","    elif use_mps:\n","        device = torch.device(\"mps\")\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","    train_kwargs = {'batch_size': args.batch_size}\n","    test_kwargs = {'batch_size': args.test_batch_size}\n","    if use_cuda:\n","        cuda_kwargs = {'num_workers': 1,\n","                       'pin_memory': True,\n","                       'shuffle': True}\n","        train_kwargs.update(cuda_kwargs)\n","        test_kwargs.update(cuda_kwargs)\n","\n","    train_dataset = APP_MATCHER('../data', train=True, download=True)\n","    # test_dataset = APP_MATCHER('../data', train=False)\n","    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n","    # test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)\n","\n","    model = SiameseNetwork().to(device)\n","    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n","\n","    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n","    for epoch in range(1, args.epochs + 1):\n","        train(args, model, device, train_loader, optimizer, epoch)\n","        # test(model, device, test_loader)\n","        scheduler.step()\n","\n","    if args.save_model:\n","        torch.save(model.state_dict(), \"siamese_network.pt\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-20T11:30:12.232751Z","iopub.status.busy":"2024-06-20T11:30:12.232487Z","iopub.status.idle":"2024-06-20T12:03:23.784679Z","shell.execute_reply":"2024-06-20T12:03:23.783588Z","shell.execute_reply.started":"2024-06-20T11:30:12.232730Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/7723 (0%)]\tLoss: 2.508582\n","Train Epoch: 1 [640/7723 (8%)]\tLoss: 1.484504\n","Train Epoch: 1 [1280/7723 (17%)]\tLoss: 1.923212\n","Train Epoch: 1 [1920/7723 (25%)]\tLoss: 1.219400\n","Train Epoch: 1 [2560/7723 (33%)]\tLoss: 1.204297\n","Train Epoch: 1 [3200/7723 (41%)]\tLoss: 0.978742\n","Train Epoch: 1 [3840/7723 (50%)]\tLoss: 1.080977\n","Train Epoch: 1 [4480/7723 (58%)]\tLoss: 1.127035\n","Train Epoch: 1 [5120/7723 (66%)]\tLoss: 1.199586\n","Train Epoch: 1 [5760/7723 (74%)]\tLoss: 1.293611\n","Train Epoch: 1 [6400/7723 (83%)]\tLoss: 0.703592\n","Train Epoch: 1 [7040/7723 (91%)]\tLoss: 0.969760\n","Train Epoch: 1 [5160/7723 (99%)]\tLoss: 0.790864\n","Train Epoch: 2 [0/7723 (0%)]\tLoss: 0.734059\n","Train Epoch: 2 [640/7723 (8%)]\tLoss: 0.824094\n","Train Epoch: 2 [1280/7723 (17%)]\tLoss: 0.732086\n","Train Epoch: 2 [1920/7723 (25%)]\tLoss: 0.672383\n","Train Epoch: 2 [2560/7723 (33%)]\tLoss: 0.819933\n","Train Epoch: 2 [3200/7723 (41%)]\tLoss: 0.471012\n","Train Epoch: 2 [3840/7723 (50%)]\tLoss: 0.568180\n","Train Epoch: 2 [4480/7723 (58%)]\tLoss: 0.685142\n","Train Epoch: 2 [5120/7723 (66%)]\tLoss: 0.629840\n","Train Epoch: 2 [5760/7723 (74%)]\tLoss: 0.389061\n","Train Epoch: 2 [6400/7723 (83%)]\tLoss: 0.655945\n","Train Epoch: 2 [7040/7723 (91%)]\tLoss: 0.552898\n","Train Epoch: 2 [5160/7723 (99%)]\tLoss: 0.554429\n","Train Epoch: 3 [0/7723 (0%)]\tLoss: 0.583825\n","Train Epoch: 3 [640/7723 (8%)]\tLoss: 0.593366\n","Train Epoch: 3 [1280/7723 (17%)]\tLoss: 0.648708\n","Train Epoch: 3 [1920/7723 (25%)]\tLoss: 0.741318\n","Train Epoch: 3 [2560/7723 (33%)]\tLoss: 0.548734\n","Train Epoch: 3 [3200/7723 (41%)]\tLoss: 0.426206\n","Train Epoch: 3 [3840/7723 (50%)]\tLoss: 0.843520\n","Train Epoch: 3 [4480/7723 (58%)]\tLoss: 0.476074\n","Train Epoch: 3 [5120/7723 (66%)]\tLoss: 0.462022\n","Train Epoch: 3 [5760/7723 (74%)]\tLoss: 0.553180\n","Train Epoch: 3 [6400/7723 (83%)]\tLoss: 0.559699\n","Train Epoch: 3 [7040/7723 (91%)]\tLoss: 0.572091\n","Train Epoch: 3 [5160/7723 (99%)]\tLoss: 0.597833\n","Train Epoch: 4 [0/7723 (0%)]\tLoss: 0.393774\n","Train Epoch: 4 [640/7723 (8%)]\tLoss: 0.398739\n","Train Epoch: 4 [1280/7723 (17%)]\tLoss: 0.562379\n","Train Epoch: 4 [1920/7723 (25%)]\tLoss: 0.355969\n","Train Epoch: 4 [2560/7723 (33%)]\tLoss: 0.626517\n","Train Epoch: 4 [3200/7723 (41%)]\tLoss: 0.321074\n","Train Epoch: 4 [3840/7723 (50%)]\tLoss: 0.400522\n","Train Epoch: 4 [4480/7723 (58%)]\tLoss: 0.465707\n","Train Epoch: 4 [5120/7723 (66%)]\tLoss: 0.438058\n","Train Epoch: 4 [5760/7723 (74%)]\tLoss: 0.371203\n","Train Epoch: 4 [6400/7723 (83%)]\tLoss: 0.412812\n","Train Epoch: 4 [7040/7723 (91%)]\tLoss: 0.392388\n","Train Epoch: 4 [5160/7723 (99%)]\tLoss: 0.618864\n","Train Epoch: 5 [0/7723 (0%)]\tLoss: 0.529091\n","Train Epoch: 5 [640/7723 (8%)]\tLoss: 0.436897\n","Train Epoch: 5 [1280/7723 (17%)]\tLoss: 0.742194\n","Train Epoch: 5 [1920/7723 (25%)]\tLoss: 0.453900\n","Train Epoch: 5 [2560/7723 (33%)]\tLoss: 0.596418\n","Train Epoch: 5 [3200/7723 (41%)]\tLoss: 0.367265\n","Train Epoch: 5 [3840/7723 (50%)]\tLoss: 0.277454\n","Train Epoch: 5 [4480/7723 (58%)]\tLoss: 0.453592\n","Train Epoch: 5 [5120/7723 (66%)]\tLoss: 0.283243\n","Train Epoch: 5 [5760/7723 (74%)]\tLoss: 0.314177\n","Train Epoch: 5 [6400/7723 (83%)]\tLoss: 0.460542\n","Train Epoch: 5 [7040/7723 (91%)]\tLoss: 0.458527\n","Train Epoch: 5 [5160/7723 (99%)]\tLoss: 0.609149\n","Train Epoch: 6 [0/7723 (0%)]\tLoss: 0.360761\n","Train Epoch: 6 [640/7723 (8%)]\tLoss: 0.483988\n","Train Epoch: 6 [1280/7723 (17%)]\tLoss: 0.405186\n","Train Epoch: 6 [1920/7723 (25%)]\tLoss: 0.264447\n","Train Epoch: 6 [2560/7723 (33%)]\tLoss: 0.407852\n","Train Epoch: 6 [3200/7723 (41%)]\tLoss: 0.639003\n","Train Epoch: 6 [3840/7723 (50%)]\tLoss: 0.313371\n","Train Epoch: 6 [4480/7723 (58%)]\tLoss: 0.515510\n","Train Epoch: 6 [5120/7723 (66%)]\tLoss: 0.404159\n","Train Epoch: 6 [5760/7723 (74%)]\tLoss: 0.353892\n","Train Epoch: 6 [6400/7723 (83%)]\tLoss: 0.390370\n","Train Epoch: 6 [7040/7723 (91%)]\tLoss: 0.440727\n","Train Epoch: 6 [5160/7723 (99%)]\tLoss: 0.404144\n","Train Epoch: 7 [0/7723 (0%)]\tLoss: 0.659395\n","Train Epoch: 7 [640/7723 (8%)]\tLoss: 0.300715\n","Train Epoch: 7 [1280/7723 (17%)]\tLoss: 0.481676\n","Train Epoch: 7 [1920/7723 (25%)]\tLoss: 0.339756\n","Train Epoch: 7 [2560/7723 (33%)]\tLoss: 0.290418\n","Train Epoch: 7 [3200/7723 (41%)]\tLoss: 0.509265\n","Train Epoch: 7 [3840/7723 (50%)]\tLoss: 0.292357\n","Train Epoch: 7 [4480/7723 (58%)]\tLoss: 0.672963\n","Train Epoch: 7 [5120/7723 (66%)]\tLoss: 0.618298\n","Train Epoch: 7 [5760/7723 (74%)]\tLoss: 0.318625\n","Train Epoch: 7 [6400/7723 (83%)]\tLoss: 0.381701\n","Train Epoch: 7 [7040/7723 (91%)]\tLoss: 0.652865\n","Train Epoch: 7 [5160/7723 (99%)]\tLoss: 0.475244\n","Train Epoch: 8 [0/7723 (0%)]\tLoss: 0.335129\n","Train Epoch: 8 [640/7723 (8%)]\tLoss: 0.524679\n","Train Epoch: 8 [1280/7723 (17%)]\tLoss: 0.363999\n","Train Epoch: 8 [1920/7723 (25%)]\tLoss: 0.445094\n","Train Epoch: 8 [2560/7723 (33%)]\tLoss: 0.339659\n","Train Epoch: 8 [3200/7723 (41%)]\tLoss: 0.445252\n","Train Epoch: 8 [3840/7723 (50%)]\tLoss: 0.482775\n","Train Epoch: 8 [4480/7723 (58%)]\tLoss: 0.494591\n","Train Epoch: 8 [5120/7723 (66%)]\tLoss: 0.213942\n","Train Epoch: 8 [5760/7723 (74%)]\tLoss: 0.432298\n","Train Epoch: 8 [6400/7723 (83%)]\tLoss: 0.199554\n","Train Epoch: 8 [7040/7723 (91%)]\tLoss: 0.458225\n","Train Epoch: 8 [5160/7723 (99%)]\tLoss: 0.273144\n","Train Epoch: 9 [0/7723 (0%)]\tLoss: 0.322235\n","Train Epoch: 9 [640/7723 (8%)]\tLoss: 0.551450\n","Train Epoch: 9 [1280/7723 (17%)]\tLoss: 0.427410\n","Train Epoch: 9 [1920/7723 (25%)]\tLoss: 0.421342\n","Train Epoch: 9 [2560/7723 (33%)]\tLoss: 0.392065\n","Train Epoch: 9 [3200/7723 (41%)]\tLoss: 0.249440\n","Train Epoch: 9 [3840/7723 (50%)]\tLoss: 0.405328\n","Train Epoch: 9 [4480/7723 (58%)]\tLoss: 0.279249\n","Train Epoch: 9 [5120/7723 (66%)]\tLoss: 0.210348\n","Train Epoch: 9 [5760/7723 (74%)]\tLoss: 0.218122\n","Train Epoch: 9 [6400/7723 (83%)]\tLoss: 0.559900\n","Train Epoch: 9 [7040/7723 (91%)]\tLoss: 0.263804\n","Train Epoch: 9 [5160/7723 (99%)]\tLoss: 0.442356\n","Train Epoch: 10 [0/7723 (0%)]\tLoss: 0.486595\n","Train Epoch: 10 [640/7723 (8%)]\tLoss: 0.177972\n","Train Epoch: 10 [1280/7723 (17%)]\tLoss: 0.256308\n","Train Epoch: 10 [1920/7723 (25%)]\tLoss: 0.463252\n","Train Epoch: 10 [2560/7723 (33%)]\tLoss: 0.340500\n","Train Epoch: 10 [3200/7723 (41%)]\tLoss: 0.420330\n","Train Epoch: 10 [3840/7723 (50%)]\tLoss: 0.558476\n","Train Epoch: 10 [4480/7723 (58%)]\tLoss: 0.415816\n","Train Epoch: 10 [5120/7723 (66%)]\tLoss: 0.441723\n","Train Epoch: 10 [5760/7723 (74%)]\tLoss: 0.288866\n","Train Epoch: 10 [6400/7723 (83%)]\tLoss: 0.349273\n","Train Epoch: 10 [7040/7723 (91%)]\tLoss: 0.461483\n","Train Epoch: 10 [5160/7723 (99%)]\tLoss: 0.515209\n","Train Epoch: 11 [0/7723 (0%)]\tLoss: 0.342274\n","Train Epoch: 11 [640/7723 (8%)]\tLoss: 0.293748\n","Train Epoch: 11 [1280/7723 (17%)]\tLoss: 0.392583\n","Train Epoch: 11 [1920/7723 (25%)]\tLoss: 0.333826\n","Train Epoch: 11 [2560/7723 (33%)]\tLoss: 0.399736\n","Train Epoch: 11 [3200/7723 (41%)]\tLoss: 0.521758\n","Train Epoch: 11 [3840/7723 (50%)]\tLoss: 0.265814\n","Train Epoch: 11 [4480/7723 (58%)]\tLoss: 0.184433\n","Train Epoch: 11 [5120/7723 (66%)]\tLoss: 0.389191\n","Train Epoch: 11 [5760/7723 (74%)]\tLoss: 0.435830\n","Train Epoch: 11 [6400/7723 (83%)]\tLoss: 0.407453\n","Train Epoch: 11 [7040/7723 (91%)]\tLoss: 0.440413\n","Train Epoch: 11 [5160/7723 (99%)]\tLoss: 0.529818\n","Train Epoch: 12 [0/7723 (0%)]\tLoss: 0.445421\n","Train Epoch: 12 [640/7723 (8%)]\tLoss: 0.361023\n","Train Epoch: 12 [1280/7723 (17%)]\tLoss: 0.417088\n","Train Epoch: 12 [1920/7723 (25%)]\tLoss: 0.503054\n","Train Epoch: 12 [2560/7723 (33%)]\tLoss: 0.488247\n","Train Epoch: 12 [3200/7723 (41%)]\tLoss: 0.312711\n","Train Epoch: 12 [3840/7723 (50%)]\tLoss: 0.517630\n","Train Epoch: 12 [4480/7723 (58%)]\tLoss: 0.380306\n","Train Epoch: 12 [5120/7723 (66%)]\tLoss: 0.388104\n","Train Epoch: 12 [5760/7723 (74%)]\tLoss: 0.201880\n","Train Epoch: 12 [6400/7723 (83%)]\tLoss: 0.238996\n","Train Epoch: 12 [7040/7723 (91%)]\tLoss: 0.341624\n","Train Epoch: 12 [5160/7723 (99%)]\tLoss: 0.508353\n","Train Epoch: 13 [0/7723 (0%)]\tLoss: 0.505377\n","Train Epoch: 13 [640/7723 (8%)]\tLoss: 0.265329\n","Train Epoch: 13 [1280/7723 (17%)]\tLoss: 0.466354\n","Train Epoch: 13 [1920/7723 (25%)]\tLoss: 0.332551\n","Train Epoch: 13 [2560/7723 (33%)]\tLoss: 0.313553\n","Train Epoch: 13 [3200/7723 (41%)]\tLoss: 0.488301\n","Train Epoch: 13 [3840/7723 (50%)]\tLoss: 0.312728\n","Train Epoch: 13 [4480/7723 (58%)]\tLoss: 0.453671\n","Train Epoch: 13 [5120/7723 (66%)]\tLoss: 0.327165\n","Train Epoch: 13 [5760/7723 (74%)]\tLoss: 0.344702\n","Train Epoch: 13 [6400/7723 (83%)]\tLoss: 0.395007\n","Train Epoch: 13 [7040/7723 (91%)]\tLoss: 0.454018\n","Train Epoch: 13 [5160/7723 (99%)]\tLoss: 0.346814\n","Train Epoch: 14 [0/7723 (0%)]\tLoss: 0.258334\n","Train Epoch: 14 [640/7723 (8%)]\tLoss: 0.252716\n","Train Epoch: 14 [1280/7723 (17%)]\tLoss: 0.401359\n","Train Epoch: 14 [1920/7723 (25%)]\tLoss: 0.244455\n","Train Epoch: 14 [2560/7723 (33%)]\tLoss: 0.409664\n","Train Epoch: 14 [3200/7723 (41%)]\tLoss: 0.357407\n","Train Epoch: 14 [3840/7723 (50%)]\tLoss: 0.261101\n","Train Epoch: 14 [4480/7723 (58%)]\tLoss: 0.468230\n","Train Epoch: 14 [5120/7723 (66%)]\tLoss: 0.291684\n","Train Epoch: 14 [5760/7723 (74%)]\tLoss: 0.297115\n","Train Epoch: 14 [6400/7723 (83%)]\tLoss: 0.315602\n","Train Epoch: 14 [7040/7723 (91%)]\tLoss: 0.247424\n","Train Epoch: 14 [5160/7723 (99%)]\tLoss: 0.491310\n","Train Epoch: 15 [0/7723 (0%)]\tLoss: 0.326031\n","Train Epoch: 15 [640/7723 (8%)]\tLoss: 0.223109\n","Train Epoch: 15 [1280/7723 (17%)]\tLoss: 0.275705\n","Train Epoch: 15 [1920/7723 (25%)]\tLoss: 0.370870\n","Train Epoch: 15 [2560/7723 (33%)]\tLoss: 0.263667\n","Train Epoch: 15 [3200/7723 (41%)]\tLoss: 0.479869\n","Train Epoch: 15 [3840/7723 (50%)]\tLoss: 0.260113\n","Train Epoch: 15 [4480/7723 (58%)]\tLoss: 0.289705\n","Train Epoch: 15 [5120/7723 (66%)]\tLoss: 0.372076\n","Train Epoch: 15 [5760/7723 (74%)]\tLoss: 0.391920\n","Train Epoch: 15 [6400/7723 (83%)]\tLoss: 0.214701\n","Train Epoch: 15 [7040/7723 (91%)]\tLoss: 0.312053\n","Train Epoch: 15 [5160/7723 (99%)]\tLoss: 0.356132\n","Train Epoch: 16 [0/7723 (0%)]\tLoss: 0.483692\n","Train Epoch: 16 [640/7723 (8%)]\tLoss: 0.350195\n","Train Epoch: 16 [1280/7723 (17%)]\tLoss: 0.308660\n","Train Epoch: 16 [1920/7723 (25%)]\tLoss: 0.433910\n","Train Epoch: 16 [2560/7723 (33%)]\tLoss: 0.337041\n","Train Epoch: 16 [3200/7723 (41%)]\tLoss: 0.328972\n","Train Epoch: 16 [3840/7723 (50%)]\tLoss: 0.499344\n","Train Epoch: 16 [4480/7723 (58%)]\tLoss: 0.176564\n","Train Epoch: 16 [5120/7723 (66%)]\tLoss: 0.416806\n","Train Epoch: 16 [5760/7723 (74%)]\tLoss: 0.305389\n","Train Epoch: 16 [6400/7723 (83%)]\tLoss: 0.366211\n","Train Epoch: 16 [7040/7723 (91%)]\tLoss: 0.310061\n","Train Epoch: 16 [5160/7723 (99%)]\tLoss: 0.354146\n","Train Epoch: 17 [0/7723 (0%)]\tLoss: 0.342699\n","Train Epoch: 17 [640/7723 (8%)]\tLoss: 0.503189\n","Train Epoch: 17 [1280/7723 (17%)]\tLoss: 0.316622\n","Train Epoch: 17 [1920/7723 (25%)]\tLoss: 0.511896\n","Train Epoch: 17 [2560/7723 (33%)]\tLoss: 0.405538\n","Train Epoch: 17 [3200/7723 (41%)]\tLoss: 0.353703\n","Train Epoch: 17 [3840/7723 (50%)]\tLoss: 0.359854\n","Train Epoch: 17 [4480/7723 (58%)]\tLoss: 0.345901\n","Train Epoch: 17 [5120/7723 (66%)]\tLoss: 0.275796\n","Train Epoch: 17 [5760/7723 (74%)]\tLoss: 0.490194\n","Train Epoch: 17 [6400/7723 (83%)]\tLoss: 0.388306\n","Train Epoch: 17 [7040/7723 (91%)]\tLoss: 0.193812\n","Train Epoch: 17 [5160/7723 (99%)]\tLoss: 0.514172\n","Train Epoch: 18 [0/7723 (0%)]\tLoss: 0.311100\n","Train Epoch: 18 [640/7723 (8%)]\tLoss: 0.551837\n","Train Epoch: 18 [1280/7723 (17%)]\tLoss: 0.383851\n","Train Epoch: 18 [1920/7723 (25%)]\tLoss: 0.367848\n","Train Epoch: 18 [2560/7723 (33%)]\tLoss: 0.261667\n","Train Epoch: 18 [3200/7723 (41%)]\tLoss: 0.379151\n","Train Epoch: 18 [3840/7723 (50%)]\tLoss: 0.472429\n","Train Epoch: 18 [4480/7723 (58%)]\tLoss: 0.409220\n","Train Epoch: 18 [5120/7723 (66%)]\tLoss: 0.224161\n","Train Epoch: 18 [5760/7723 (74%)]\tLoss: 0.342147\n","Train Epoch: 18 [6400/7723 (83%)]\tLoss: 0.384102\n","Train Epoch: 18 [7040/7723 (91%)]\tLoss: 0.414629\n","Train Epoch: 18 [5160/7723 (99%)]\tLoss: 0.153040\n","Train Epoch: 19 [0/7723 (0%)]\tLoss: 0.518705\n","Train Epoch: 19 [640/7723 (8%)]\tLoss: 0.298606\n","Train Epoch: 19 [1280/7723 (17%)]\tLoss: 0.507584\n","Train Epoch: 19 [1920/7723 (25%)]\tLoss: 0.446056\n","Train Epoch: 19 [2560/7723 (33%)]\tLoss: 0.353242\n","Train Epoch: 19 [3200/7723 (41%)]\tLoss: 0.341226\n","Train Epoch: 19 [3840/7723 (50%)]\tLoss: 0.273697\n","Train Epoch: 19 [4480/7723 (58%)]\tLoss: 0.338749\n","Train Epoch: 19 [5120/7723 (66%)]\tLoss: 0.519639\n","Train Epoch: 19 [5760/7723 (74%)]\tLoss: 0.329358\n","Train Epoch: 19 [6400/7723 (83%)]\tLoss: 0.705858\n","Train Epoch: 19 [7040/7723 (91%)]\tLoss: 0.489936\n","Train Epoch: 19 [5160/7723 (99%)]\tLoss: 0.456400\n","Train Epoch: 20 [0/7723 (0%)]\tLoss: 0.606557\n","Train Epoch: 20 [640/7723 (8%)]\tLoss: 0.299404\n","Train Epoch: 20 [1280/7723 (17%)]\tLoss: 0.368288\n","Train Epoch: 20 [1920/7723 (25%)]\tLoss: 0.319540\n","Train Epoch: 20 [2560/7723 (33%)]\tLoss: 0.384475\n","Train Epoch: 20 [3200/7723 (41%)]\tLoss: 0.439358\n","Train Epoch: 20 [3840/7723 (50%)]\tLoss: 0.313848\n","Train Epoch: 20 [4480/7723 (58%)]\tLoss: 0.315851\n","Train Epoch: 20 [5120/7723 (66%)]\tLoss: 0.336559\n","Train Epoch: 20 [5760/7723 (74%)]\tLoss: 0.337882\n","Train Epoch: 20 [6400/7723 (83%)]\tLoss: 0.364628\n","Train Epoch: 20 [7040/7723 (91%)]\tLoss: 0.428135\n","Train Epoch: 20 [5160/7723 (99%)]\tLoss: 0.416075\n"]}],"source":["main()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5231641,"sourceId":8719138,"sourceType":"datasetVersion"},{"datasetId":5243125,"sourceId":8734541,"sourceType":"datasetVersion"},{"datasetId":5244078,"sourceId":8735758,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
